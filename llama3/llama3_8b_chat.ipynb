{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30360be6-45a0-4720-acb5-b7e30428af17",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Llama3 大模型微调实战"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b971df16-ede1-4289-8dce-643292b83a9f",
   "metadata": {},
   "source": [
    "Llama3 是 Meta 于2024年4月开放的 Llama 系列的最新模型。基于超过 15T token 训练，相当于 Llama 2 数据集的 7 倍多；支持 8K 长文本，改进的 tokenizer 具有 128K token 的词汇量，可实现更好的性能。Llama3 提供两个版本: 8B版本适合在消费级GPU上高效部署和开发，70B版本则专为大规模AI应用设计。每个版本都包括基础和指令调优两种形式：\n",
    "* Meta-Llama-3-8b: 8B 基础模型\n",
    "* Meta-Llama-3-8b-instruct: 8B 基础模型的指令调优版\n",
    "* Meta-Llama-3-70b: 70B 基础模型\n",
    "* Meta-Llama-3-70b-instruct: 70B 基础模型的指令调优版\n",
    "\n",
    "本文将以Llama-3-8B-Instruct为例，为您介绍如何在PAI-DSW中微调Llama3大模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9b693c-3081-48b4-8cb7-6f7be98a7b0e",
   "metadata": {},
   "source": [
    "## 运行环境要求"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7669e9df-5fc1-4e1a-a7f6-aa07aef8cdc2",
   "metadata": {},
   "source": [
    "* Python环境3.9以上，推荐使用V100(16GB)或更高显存的GPU。\n",
    "\n",
    "* 镜像推荐使用如下URL，其中REGION为DSW实例所属区域，例如cn-shanghai、cn-hangzhou等。\n",
    "\n",
    "dsw-registry-vpc.REGION.cr.aliyuncs.com/pai-training-algorithm/llm_deepspeed_peft:v0.0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc5a1cf-1275-45b8-ae36-6fd03609ad09",
   "metadata": {},
   "source": [
    "## 准备工作\n",
    "### 下载模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd4c04e-5bc0-4a59-a8c6-a3861f62a107",
   "metadata": {},
   "source": [
    "**注：使用此模型受Meta许可证的约束。在使用模型前，请务必阅读[Meta官方许可证](https://huggingface.co/meta-llama/Meta-Llama-3-70B/blob/main/LICENSE)。**\n",
    "\n",
    "首先，需要下载模型，您可以[向Meta申请下载模型](https://llama.meta.com/llama-downloads)，或者根据下文代码通过ModelScope下载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "136514ca-ec6e-47c2-addc-4ea2e2f1aa77",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-05-02T06:57:19.899445Z",
     "iopub.status.busy": "2024-05-02T06:57:19.898998Z",
     "iopub.status.idle": "2024-05-02T06:57:35.775889Z",
     "shell.execute_reply": "2024-05-02T06:57:35.775091Z",
     "shell.execute_reply.started": "2024-05-02T06:57:19.899408Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: modelscope==1.12.0 in /opt/conda/lib/python3.10/site-packages (1.12.0)\n",
      "Collecting transformers==4.37.0\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/3c/45/52133ce6bce49a099cc865599803bf1fad93de887276f728e56848d77a70/transformers-4.37.0-py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: addict in /opt/conda/lib/python3.10/site-packages (from modelscope==1.12.0) (2.4.0)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.10/site-packages (from modelscope==1.12.0) (23.2.0)\n",
      "Requirement already satisfied: datasets>=2.14.5 in /opt/conda/lib/python3.10/site-packages (from modelscope==1.12.0) (2.16.1)\n",
      "Requirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from modelscope==1.12.0) (0.7.0)\n",
      "Requirement already satisfied: filelock>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from modelscope==1.12.0) (3.13.1)\n",
      "Requirement already satisfied: gast>=0.2.2 in /opt/conda/lib/python3.10/site-packages (from modelscope==1.12.0) (0.5.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from modelscope==1.12.0) (1.26.3)\n",
      "Requirement already satisfied: oss2 in /opt/conda/lib/python3.10/site-packages (from modelscope==1.12.0) (2.18.4)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from modelscope==1.12.0) (2.2.0)\n",
      "Requirement already satisfied: Pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from modelscope==1.12.0) (10.2.0)\n",
      "Requirement already satisfied: pyarrow!=9.0.0,>=6.0.0 in /opt/conda/lib/python3.10/site-packages (from modelscope==1.12.0) (15.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.10/site-packages (from modelscope==1.12.0) (2.8.2)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from modelscope==1.12.0) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.25 in /opt/conda/lib/python3.10/site-packages (from modelscope==1.12.0) (2.31.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from modelscope==1.12.0) (1.11.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from modelscope==1.12.0) (68.0.0)\n",
      "Requirement already satisfied: simplejson>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from modelscope==1.12.0) (3.19.2)\n",
      "Requirement already satisfied: sortedcontainers>=1.5.9 in /opt/conda/lib/python3.10/site-packages (from modelscope==1.12.0) (2.4.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.10/site-packages (from modelscope==1.12.0) (4.65.0)\n",
      "Requirement already satisfied: urllib3>=1.26 in /opt/conda/lib/python3.10/site-packages (from modelscope==1.12.0) (1.26.16)\n",
      "Requirement already satisfied: yapf in /opt/conda/lib/python3.10/site-packages (from modelscope==1.12.0) (0.30.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.37.0) (0.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.37.0) (23.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.37.0) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers==4.37.0) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.37.0) (0.4.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.5->modelscope==1.12.0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.5->modelscope==1.12.0) (0.3.7)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.5->modelscope==1.12.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.5->modelscope==1.12.0) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets>=2.14.5->modelscope==1.12.0) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.5->modelscope==1.12.0) (3.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.0) (4.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.1->modelscope==1.12.0) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25->modelscope==1.12.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25->modelscope==1.12.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25->modelscope==1.12.0) (2023.11.17)\n",
      "Requirement already satisfied: crcmod>=1.7 in /opt/conda/lib/python3.10/site-packages (from oss2->modelscope==1.12.0) (1.7)\n",
      "Requirement already satisfied: pycryptodome>=3.4.7 in /opt/conda/lib/python3.10/site-packages (from oss2->modelscope==1.12.0) (3.20.0)\n",
      "Requirement already satisfied: aliyun-python-sdk-kms>=2.4.1 in /opt/conda/lib/python3.10/site-packages (from oss2->modelscope==1.12.0) (2.16.2)\n",
      "Requirement already satisfied: aliyun-python-sdk-core>=2.13.12 in /opt/conda/lib/python3.10/site-packages (from oss2->modelscope==1.12.0) (2.14.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->modelscope==1.12.0) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->modelscope==1.12.0) (2023.4)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.9.3 in /opt/conda/lib/python3.10/site-packages (from aliyun-python-sdk-core>=2.13.12->oss2->modelscope==1.12.0) (0.10.0)\n",
      "Requirement already satisfied: cryptography>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from aliyun-python-sdk-core>=2.13.12->oss2->modelscope==1.12.0) (41.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.5->modelscope==1.12.0) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.5->modelscope==1.12.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.5->modelscope==1.12.0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.5->modelscope==1.12.0) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.5->modelscope==1.12.0) (4.0.3)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=2.6.0->aliyun-python-sdk-core>=2.13.12->oss2->modelscope==1.12.0) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.6.0->aliyun-python-sdk-core>=2.13.12->oss2->modelscope==1.12.0) (2.21)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.37.2\n",
      "    Uninstalling transformers-4.37.2:\n",
      "      Successfully uninstalled transformers-4.37.2\n",
      "Successfully installed transformers-4.37.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install modelscope==1.12.0 transformers==4.37.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59157807-e7ee-4d42-a695-b77f028d4bdd",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-05-02T06:57:45.084999Z",
     "iopub.status.busy": "2024-05-02T06:57:45.084534Z",
     "iopub.status.idle": "2024-05-02T06:59:43.872100Z",
     "shell.execute_reply": "2024-05-02T06:59:43.871367Z",
     "shell.execute_reply.started": "2024-05-02T06:57:45.084969Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 14:57:49,459 - modelscope - INFO - PyTorch version 2.1.2+cu121 Found.\n",
      "2024-05-02 14:57:49,463 - modelscope - INFO - TensorFlow version 2.14.0 Found.\n",
      "2024-05-02 14:57:49,464 - modelscope - INFO - Loading ast index from /mnt/workspace/.cache/modelscope/ast_indexer\n",
      "2024-05-02 14:57:49,464 - modelscope - INFO - No valid ast index found from /mnt/workspace/.cache/modelscope/ast_indexer, generating ast index from prebuilt!\n",
      "2024-05-02 14:57:50,061 - modelscope - INFO - Loading done! Current index file version is 1.12.0, with md5 509123dba36c5e70a95f6780df348471 and a total number of 964 components indexed\n",
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: 100%|██████████| 654/654 [00:00<00:00, 3.61MB/s]\n",
      "Downloading: 100%|██████████| 48.0/48.0 [00:00<00:00, 320kB/s]\n",
      "Downloading: 100%|██████████| 187/187 [00:00<00:00, 1.08MB/s]\n",
      "Downloading: 100%|██████████| 7.62k/7.62k [00:00<00:00, 28.8MB/s]\n",
      "Downloading: 100%|█████████▉| 4.63G/4.63G [00:16<00:00, 301MB/s]\n",
      "Downloading: 100%|█████████▉| 4.66G/4.66G [00:16<00:00, 295MB/s]\n",
      "Downloading: 100%|█████████▉| 4.58G/4.58G [00:18<00:00, 269MB/s]\n",
      "Downloading: 100%|█████████▉| 1.09G/1.09G [00:04<00:00, 254MB/s]\n",
      "Downloading: 100%|██████████| 23.4k/23.4k [00:00<00:00, 22.8MB/s]\n",
      "Downloading: 100%|██████████| 36.3k/36.3k [00:00<00:00, 11.4MB/s]\n",
      "Downloading: 100%|██████████| 73.0/73.0 [00:00<00:00, 384kB/s]\n",
      "Downloading: 100%|██████████| 8.66M/8.66M [00:00<00:00, 43.2MB/s]\n",
      "Downloading: 100%|██████████| 49.8k/49.8k [00:00<00:00, 12.7MB/s]\n",
      "Downloading: 100%|██████████| 4.59k/4.59k [00:00<00:00, 8.64MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./LLM-Research/Meta-Llama-3-8B-Instruct'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modelscope.hub.snapshot_download import snapshot_download\n",
    "snapshot_download('LLM-Research/Meta-Llama-3-8B-Instruct', cache_dir='.', revision='master')\n",
    "# snapshot_download('LLM-Research/Meta-Llama-3-8B', cache_dir='.', revision='master')\n",
    "# snapshot_download('LLM-Research/Meta-Llama-3-70B-Instruct', cache_dir='.', revision='master')\n",
    "# snapshot_download('LLM-Research/Meta-Llama-3-70B', cache_dir='.', revision='master')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008f6f7a-0a1b-4bef-aca3-e1f5495b7eae",
   "metadata": {},
   "source": [
    "### 准备数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb02457-4ddb-4080-b474-e1cabe045070",
   "metadata": {},
   "source": [
    "接下来，准备微调示例数据集。本文准备了英文诗歌数据集，来微调 Llama3 大模型，使其提高其生成诗歌的表现能力。\n",
    "\n",
    "您也可以参考该数据集的格式，根据自己的使用场景，准备所需的数据集。通过微调，能够提高大语言模型在特定任务上的回答准确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70d13c80-7dab-425e-b419-210cfc960244",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-05-02T07:00:08.393830Z",
     "iopub.status.busy": "2024-05-02T07:00:08.393184Z",
     "iopub.status.idle": "2024-05-02T07:00:09.192907Z",
     "shell.execute_reply": "2024-05-02T07:00:09.192092Z",
     "shell.execute_reply.started": "2024-05-02T07:00:08.393796Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-02 15:00:08--  https://atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com/tutorial/llm_instruct/en_poetry_train.json\n",
      "正在解析主机 atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com (atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com)... 47.101.88.27\n",
      "正在连接 atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com (atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com)|47.101.88.27|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度： 636887 (622K) [application/json]\n",
      "正在保存至: ‘en_poetry_train.json’\n",
      "\n",
      "en_poetry_train.jso 100%[===================>] 621.96K  --.-KB/s    用时 0.02s   \n",
      "\n",
      "2024-05-02 15:00:09 (24.8 MB/s) - 已保存 ‘en_poetry_train.json’ [636887/636887])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com/tutorial/llm_instruct/en_poetry_train.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffa4922-1347-421a-a22f-265efb0d7715",
   "metadata": {},
   "source": [
    "## 微调模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d194563-3e4a-46f5-9a91-aa22b8b0eee3",
   "metadata": {},
   "source": [
    "接下来，基于已有的训练脚本`/ml/code/sft.py`，进行模型的LoRA轻量化训练。在训练结束之后，我们将模型参数进行量化，以便使用更少显存进行推理。\n",
    "\n",
    "示例使用的参数解释如下，请您根据实际情况进行修改：\n",
    "\n",
    "- `accelerate launch`命令行工具用于在多GPU中启动和管理深度学习训练脚本。\n",
    "    - `num_processes` 1：设置并行处理的进程数量为 1，即不进行多进程并行处理。\n",
    "    - `config_file` `/ml/code/multi_gpu.yaml`：指定配置文件的路径。\n",
    "    - `/ml/code/sft.py`：指定要运行的 Python 脚本的路径。\n",
    "    \n",
    "    \n",
    "脚本`/ml/code/sft.py`接受的参数：\n",
    "- `--model_name` `./LLM-Research/Meta-Llama-3-8B-Instruct/`：指定预训练模型的路径。\n",
    "- `--model_type` `llama`：指定模型的类型，这里是 llama。\n",
    "- `--train_dataset_name` `chinese_medical_train_sampled.json`：指定训练数据集的路径。\n",
    "- `--num_train_epochs` 3：设置训练的轮次为 3。\n",
    "- `--batch_size` 8：设置批处理的大小为 8。\n",
    "- `--seq_length` 128：设置序列的长度为 128。\n",
    "- `--learning_rate` 5e-4：设置学习率为 0.0005。\n",
    "- `--lr_scheduler_type` linear：设置学习率调度器类型为线性。\n",
    "- `--target_modules` k_proj o_proj q_proj v_proj：指定在微调中需要特别关注的模型模块。\n",
    "- `--output_dir` `lora_model/`：指定输出目录路径，微调后的模型将被保存在这里。\n",
    "- `--apply_chat_template`：指定训练时应用聊天模板。\n",
    "- `--use_peft`：在训练过程中使用参数有效调优PEFT（Parameter-Efficient Fine-Tuning）方法。\n",
    "- `--load_in_4bit`：指示模型权重载入时使用 4 位精度，减少内存消耗。\n",
    "- `--peft_lora_r` 32：如果使用了 LoRA（Low-Rank Adaptation）作为参数有效调优方法的一部分，这会指定 LoRA 的秩为 32。\n",
    "- `--peft_lora_alpha` 32：设置 LoRA 参数的另一部分，alpha 的大小为 32。\n",
    "\n",
    "当运行`accelerate launch`命令时，它会使用这些参数来启动指定的 Python 脚本，并且根据`multi_gpu.yaml`配置文件中的设置，在计算资源允许的范围内进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb7e77ee-a9e0-46dd-994a-338fb2368b6c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-05-02T08:25:35.181866Z",
     "iopub.status.busy": "2024-05-02T08:25:35.181398Z",
     "iopub.status.idle": "2024-05-02T08:39:05.342504Z",
     "shell.execute_reply": "2024-05-02T08:39:05.341700Z",
     "shell.execute_reply.started": "2024-05-02T08:25:35.181837Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-02 16:25:43.704595: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-02 16:25:44.195998: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-02 16:25:44.196038: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-02 16:25:44.199348: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-02 16:25:44.497531: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-02 16:25:44.499983: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-02 16:25:46.409497: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[2024-05-02 16:25:51,431] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-05-02 16:25:52,195] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-02 16:25:52,196] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "Detected kernel version 4.19.24, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:11<00:00,  2.99s/it]\n",
      "Generating train split: 573 examples [00:00, 24556.66 examples/s]\n",
      "Map: 100%|███████████████████████████| 573/573 [00:00<00:00, 4077.36 examples/s]\n",
      "Detected kernel version 4.19.24, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/py310_cu121/cpu_adam...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/4] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o \n",
      "[2/4] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o \n",
      "[3/4] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o \n",
      "[4/4] c++ cpu_adam.o cpu_adam_impl.o custom_cuda_kernel.cuda.o -shared -lcurand -L/opt/conda/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 50.820040225982666 seconds\n",
      "  0%|                                                    | 0/12 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:226: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1947: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  overflow_gpu = get_accelerator().ByteTensor([overflow])\n",
      "{'loss': 4.0543, 'learning_rate': 0.0, 'epoch': 0.22}                           \n",
      "{'loss': 4.1022, 'learning_rate': 0.0, 'epoch': 0.44}                           \n",
      "{'loss': 4.1081, 'learning_rate': 0.0, 'epoch': 0.67}                           \n",
      "{'loss': 3.9276, 'learning_rate': 0.0, 'epoch': 0.89}                           \n",
      "{'loss': 4.1906, 'learning_rate': 0.0, 'epoch': 1.11}                           \n",
      "{'loss': 4.1446, 'learning_rate': 0.0, 'epoch': 1.33}                           \n",
      "{'loss': 4.005, 'learning_rate': 0.0, 'epoch': 1.56}                            \n",
      "{'loss': 4.0067, 'learning_rate': 0.0, 'epoch': 1.78}                           \n",
      "{'loss': 4.0537, 'learning_rate': 0.0, 'epoch': 2.0}                            \n",
      "{'loss': 3.9791, 'learning_rate': 0.0, 'epoch': 2.22}                           \n",
      "{'loss': 4.1578, 'learning_rate': 0.0, 'epoch': 2.44}                           \n",
      "{'loss': 4.036, 'learning_rate': 0.0, 'epoch': 2.67}                            \n",
      "{'train_runtime': 702.4631, 'train_samples_per_second': 2.447, 'train_steps_per_second': 0.017, 'train_loss': 4.0638194878896075, 'epoch': 2.67}\n",
      "100%|███████████████████████████████████████████| 12/12 [11:42<00:00, 58.54s/it]\n"
     ]
    }
   ],
   "source": [
    "! accelerate launch --num_processes 1 --config_file ../ml-code/multi_gpu.yaml ../ml-code/sft.py \\\n",
    "    --model_name  ./LLM-Research/Meta-Llama-3-8B-Instruct/ \\\n",
    "    --model_type llama \\\n",
    "    --train_dataset_name en_poetry_train.json \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --batch_size 8 \\\n",
    "    --seq_length 128 \\\n",
    "    --learning_rate 5e-4 \\\n",
    "    --lr_scheduler_type linear \\\n",
    "    --target_modules k_proj o_proj q_proj v_proj \\\n",
    "    --output_dir lora_model/ \\\n",
    "    --apply_chat_template \\\n",
    "    --use_peft \\\n",
    "    --load_in_4bit \\\n",
    "    --peft_lora_r 32 \\\n",
    "    --peft_lora_alpha 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a9df81-11e6-4212-af86-4bbcd977cc8e",
   "metadata": {},
   "source": [
    "接下来，将LoRA权重与基础模型融合。示例使用的参数解释如下：\n",
    "\n",
    "* RANK=0：环境变量RANK用于分布式训练中，来表示当前进程在所有进程中的序号。设为0表明它是单进程或者是分布式训练中的主进程。\n",
    "* python `/ml/code/convert.py`：执行convert.py脚本，用于权重转换或其他转换工作。\n",
    "* `--model_name` `./LLM-Research/Meta-Llama-3-8B-Instruct/`：指定基础模型的路径。\n",
    "* `--model_type` llama：指定模型类型，这里是llama。\n",
    "* `--output_dir` `trained_model/`：指定转换后的模型和权重应该输出保存的目录。\n",
    "* `--adapter_dir` `lora_model/`：指定包含LoRA适配器权重的目录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5179f9b-4405-4feb-9580-95bd2c019327",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-05-02T08:44:57.816622Z",
     "iopub.status.busy": "2024-05-02T08:44:57.816147Z",
     "iopub.status.idle": "2024-05-02T08:46:42.705915Z",
     "shell.execute_reply": "2024-05-02T08:46:42.704916Z",
     "shell.execute_reply.started": "2024-05-02T08:44:57.816583Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:04<00:00,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "! RANK=0 python ../ml-code/convert.py \\\n",
    "    --model_name ./LLM-Research/Meta-Llama-3-8B-Instruct/ \\\n",
    "    --model_type llama \\\n",
    "    --output_dir trained_model/ \\\n",
    "    --adapter_dir lora_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a3911a-d09e-4598-8c8d-6c875ee62c54",
   "metadata": {},
   "source": [
    "## 推理模型 \n",
    "\n",
    "微调好模型之后，我们使用模型进行推理，来验证微调的效果。这里我们让模型写一首关于春天的诗歌，生成的作品效果也很好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed05b0a6-0dee-464b-a52d-bb6952eb10dd",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-05-02T08:54:03.243492Z",
     "iopub.status.busy": "2024-05-02T08:54:03.243024Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  50%|█████     | 2/4 [00:55<00:55, 27.71s/it]"
     ]
    }
   ],
   "source": [
    "import torch, transformers\n",
    "\n",
    "# model_id = \"./LLM-Research/Meta-Llama-3-8B-Instruct/\"\n",
    "model_id = \"./trained_model/\"\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Write a poem on a topic 'Spring' \"},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=1024,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a856482e-2679-42f9-b7db-020e7e356a3d",
   "metadata": {},
   "source": [
    "## 部署模型\n",
    "\n",
    "您可以将微调后的模型权重上传至OSS，参考[5分钟使用EAS一键部署LLM大语言模型应用](https://help.aliyun.com/zh/pai/use-cases/deploy-llm-in-eas?spm=a2c4g.11186623.0.0.43c15699O8N4hA)，使用EAS ChatLLM部署微调后的Llama3模型服务。"
   ]
  }
 ],
 "metadata": {
  "dsw_sample": {
   "buildId": "763",
   "pipeline": "pai-dsw-examples-master"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}