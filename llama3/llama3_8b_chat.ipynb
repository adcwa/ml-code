{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30360be6-45a0-4720-acb5-b7e30428af17",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Llama3 大模型微调实战"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b971df16-ede1-4289-8dce-643292b83a9f",
   "metadata": {},
   "source": [
    "Llama3 是 Meta 于2024年4月开放的 Llama 系列的最新模型。基于超过 15T token 训练，相当于 Llama 2 数据集的 7 倍多；支持 8K 长文本，改进的 tokenizer 具有 128K token 的词汇量，可实现更好的性能。Llama3 提供两个版本: 8B版本适合在消费级GPU上高效部署和开发，70B版本则专为大规模AI应用设计。每个版本都包括基础和指令调优两种形式：\n",
    "* Meta-Llama-3-8b: 8B 基础模型\n",
    "* Meta-Llama-3-8b-instruct: 8B 基础模型的指令调优版\n",
    "* Meta-Llama-3-70b: 70B 基础模型\n",
    "* Meta-Llama-3-70b-instruct: 70B 基础模型的指令调优版\n",
    "\n",
    "本文将以Llama-3-8B-Instruct为例，为您介绍如何在PAI-DSW中微调Llama3大模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9b693c-3081-48b4-8cb7-6f7be98a7b0e",
   "metadata": {},
   "source": [
    "## 运行环境要求"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7669e9df-5fc1-4e1a-a7f6-aa07aef8cdc2",
   "metadata": {},
   "source": [
    "* Python环境3.9以上，推荐使用V100(16GB)或更高显存的GPU。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc5a1cf-1275-45b8-ae36-6fd03609ad09",
   "metadata": {},
   "source": [
    "## 准备工作\n",
    "### 下载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136514ca-ec6e-47c2-addc-4ea2e2f1aa77",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-05-02T06:57:19.899445Z",
     "iopub.status.busy": "2024-05-02T06:57:19.898998Z",
     "iopub.status.idle": "2024-05-02T06:57:35.775889Z",
     "shell.execute_reply": "2024-05-02T06:57:35.775091Z",
     "shell.execute_reply.started": "2024-05-02T06:57:19.899408Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install modelscope==1.12.0 transformers==4.37.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59157807-e7ee-4d42-a695-b77f028d4bdd",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-05-02T06:57:45.084999Z",
     "iopub.status.busy": "2024-05-02T06:57:45.084534Z",
     "iopub.status.idle": "2024-05-02T06:59:43.872100Z",
     "shell.execute_reply": "2024-05-02T06:59:43.871367Z",
     "shell.execute_reply.started": "2024-05-02T06:57:45.084969Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from modelscope.hub.snapshot_download import snapshot_download\n",
    "snapshot_download('LLM-Research/Meta-Llama-3-8B-Instruct', cache_dir='.', revision='master')\n",
    "# snapshot_download('LLM-Research/Meta-Llama-3-8B', cache_dir='.', revision='master')\n",
    "# snapshot_download('LLM-Research/Meta-Llama-3-70B-Instruct', cache_dir='.', revision='master')\n",
    "# snapshot_download('LLM-Research/Meta-Llama-3-70B', cache_dir='.', revision='master')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008f6f7a-0a1b-4bef-aca3-e1f5495b7eae",
   "metadata": {},
   "source": [
    "### 准备数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb02457-4ddb-4080-b474-e1cabe045070",
   "metadata": {},
   "source": [
    "接下来，准备微调示例数据集。本文准备了英文诗歌数据集，来微调 Llama3 大模型，使其提高其生成诗歌的表现能力。\n",
    "\n",
    "您也可以参考该数据集的格式，根据自己的使用场景，准备所需的数据集。通过微调，能够提高大语言模型在特定任务上的回答准确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d13c80-7dab-425e-b419-210cfc960244",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-05-02T07:00:08.393830Z",
     "iopub.status.busy": "2024-05-02T07:00:08.393184Z",
     "iopub.status.idle": "2024-05-02T07:00:09.192907Z",
     "shell.execute_reply": "2024-05-02T07:00:09.192092Z",
     "shell.execute_reply.started": "2024-05-02T07:00:08.393796Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com/tutorial/llm_instruct/en_poetry_train.json -O ../dataset/en_poetry_train.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffa4922-1347-421a-a22f-265efb0d7715",
   "metadata": {},
   "source": [
    "## 微调模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d194563-3e4a-46f5-9a91-aa22b8b0eee3",
   "metadata": {},
   "source": [
    "接下来，基于已有的训练脚本`../sft.py`，进行模型的LoRA轻量化训练。在训练结束之后，我们将模型参数进行量化，以便使用更少显存进行推理。\n",
    "\n",
    "示例使用的参数解释如下，请您根据实际情况进行修改：\n",
    "\n",
    "- `accelerate launch`命令行工具用于在多GPU中启动和管理深度学习训练脚本。\n",
    "    - `num_processes` 1：设置并行处理的进程数量为 1，即不进行多进程并行处理。\n",
    "    - `config_file` `../multi_gpu.yaml`：指定配置文件的路径。\n",
    "    - `../sft.py`：指定要运行的 Python 脚本的路径。\n",
    "    \n",
    "    \n",
    "脚本`../sft.py`接受的参数：\n",
    "- `--model_name` `./LLM-Research/Meta-Llama-3-8B-Instruct/`：指定预训练模型的路径。\n",
    "- `--model_type` `llama`：指定模型的类型，这里是 llama。\n",
    "- `--train_dataset_name` `chinese_medical_train_sampled.json`：指定训练数据集的路径。\n",
    "- `--num_train_epochs` 3：设置训练的轮次为 3。\n",
    "- `--batch_size` 8：设置批处理的大小为 8。\n",
    "- `--seq_length` 128：设置序列的长度为 128。\n",
    "- `--learning_rate` 5e-4：设置学习率为 0.0005。\n",
    "- `--lr_scheduler_type` linear：设置学习率调度器类型为线性。\n",
    "- `--target_modules` k_proj o_proj q_proj v_proj：指定在微调中需要特别关注的模型模块。\n",
    "- `--output_dir` `lora_model/`：指定输出目录路径，微调后的模型将被保存在这里。\n",
    "- `--apply_chat_template`：指定训练时应用聊天模板。\n",
    "- `--use_peft`：在训练过程中使用参数有效调优PEFT（Parameter-Efficient Fine-Tuning）方法。\n",
    "- `--load_in_4bit`：指示模型权重载入时使用 4 位精度，减少内存消耗。\n",
    "- `--peft_lora_r` 32：如果使用了 LoRA（Low-Rank Adaptation）作为参数有效调优方法的一部分，这会指定 LoRA 的秩为 32。\n",
    "- `--peft_lora_alpha` 32：设置 LoRA 参数的另一部分，alpha 的大小为 32。\n",
    "\n",
    "当运行`accelerate launch`命令时，它会使用这些参数来启动指定的 Python 脚本，并且根据`multi_gpu.yaml`配置文件中的设置，在计算资源允许的范围内进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7e77ee-a9e0-46dd-994a-338fb2368b6c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-05-02T08:25:35.181866Z",
     "iopub.status.busy": "2024-05-02T08:25:35.181398Z",
     "iopub.status.idle": "2024-05-02T08:39:05.342504Z",
     "shell.execute_reply": "2024-05-02T08:39:05.341700Z",
     "shell.execute_reply.started": "2024-05-02T08:25:35.181837Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! accelerate launch --num_processes 1 --config_file ../multi_gpu.yaml ../sft.py \\\n",
    "    --model_name  ./LLM-Research/Meta-Llama-3-8B-Instruct/ \\\n",
    "    --model_type llama \\\n",
    "    --train_dataset_name ../dataset/en_poetry_train.json \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --batch_size 8 \\\n",
    "    --seq_length 128 \\\n",
    "    --learning_rate 5e-4 \\\n",
    "    --lr_scheduler_type linear \\\n",
    "    --target_modules k_proj o_proj q_proj v_proj \\\n",
    "    --output_dir lora_model/ \\\n",
    "    --apply_chat_template \\\n",
    "    --use_peft \\\n",
    "    --load_in_4bit \\\n",
    "    --peft_lora_r 32 \\\n",
    "    --peft_lora_alpha 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a9df81-11e6-4212-af86-4bbcd977cc8e",
   "metadata": {},
   "source": [
    "接下来，将LoRA权重与基础模型融合。示例使用的参数解释如下：\n",
    "\n",
    "* RANK=0：环境变量RANK用于分布式训练中，来表示当前进程在所有进程中的序号。设为0表明它是单进程或者是分布式训练中的主进程。\n",
    "* python `../convert.py`：执行convert.py脚本，用于权重转换或其他转换工作。\n",
    "* `--model_name` `./LLM-Research/Meta-Llama-3-8B-Instruct/`：指定基础模型的路径。\n",
    "* `--model_type` llama：指定模型类型，这里是llama。\n",
    "* `--output_dir` `trained_model/`：指定转换后的模型和权重应该输出保存的目录。\n",
    "* `--adapter_dir` `lora_model/`：指定包含LoRA适配器权重的目录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5179f9b-4405-4feb-9580-95bd2c019327",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-05-02T08:44:57.816622Z",
     "iopub.status.busy": "2024-05-02T08:44:57.816147Z",
     "iopub.status.idle": "2024-05-02T08:46:42.705915Z",
     "shell.execute_reply": "2024-05-02T08:46:42.704916Z",
     "shell.execute_reply.started": "2024-05-02T08:44:57.816583Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:04<00:00,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "! RANK=0 python ../convert.py \\\n",
    "    --model_name ./LLM-Research/Meta-Llama-3-8B-Instruct/ \\\n",
    "    --model_type llama \\\n",
    "    --output_dir trained_model/ \\\n",
    "    --adapter_dir lora_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a3911a-d09e-4598-8c8d-6c875ee62c54",
   "metadata": {},
   "source": [
    "## 推理模型 \n",
    "\n",
    "微调好模型之后，我们使用模型进行推理，来验证微调的效果。这里我们让模型写一首关于春天的诗歌，生成的作品效果也很好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed05b0a6-0dee-464b-a52d-bb6952eb10dd",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-05-02T08:54:03.243492Z",
     "iopub.status.busy": "2024-05-02T08:54:03.243024Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch, transformers\n",
    "\n",
    "# model_id = \"./LLM-Research/Meta-Llama-3-8B-Instruct/\"\n",
    "model_id = \"./trained_model/\"\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Write a poem on a topic 'finetune' \"},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=1024,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  }
 ],
 "metadata": {
  "dsw_sample": {
   "buildId": "763",
   "pipeline": "pai-dsw-examples-master"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
